{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2c4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import permutations \n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_recall_fscore_support, hamming_loss, jaccard_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import trimesh\n",
    "\n",
    "from moduler import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/users2/yxiao11/mangoDB/wirehead')\n",
    "from wirehead import WireheadGenerator\n",
    "from wirehead import MongoTupleheadDataset, MongoheadDataset\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ff05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e1655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c076c2c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4a9210",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edf49e8a",
   "metadata": {
    "code_folding": [
     1,
     73,
     133
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class get_dataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_dir = labels_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "#         print(idx)\n",
    "        ipt = torch.from_numpy(np.load(self.data_dir[idx]))\n",
    "        ipt = ipt.permute(2, 0, 1)\n",
    "        \n",
    "        label_index = torch.from_numpy(np.load(self.labels_dir[idx]))-1\n",
    "        \n",
    "        label = torch.zeros(19)\n",
    "        label[label_index] = 1\n",
    "        \n",
    "        return ipt.float(), label.float()\n",
    "    \n",
    "\n",
    "class CubeModel(nn.Module):\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super(CubeModel, self).__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # First convolutional block\n",
    "        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 128 â†’ 64\n",
    "\n",
    "#         Second convolutional block\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 64 â†’ 32\n",
    "\n",
    "        # Third convolutional block (new)\n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 32 â†’ 16\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)  # Reduced dropout for stability\n",
    "\n",
    "        # Global Average Pooling (removes dependence on input size)\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128, 128)\n",
    "        self.fc2 = nn.Linear(128, output_dim * 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(x))))  \n",
    "        x = self.dropout(x)\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))  \n",
    "        x = self.dropout(x)\n",
    "        x = self.pool3(F.relu(self.bn3(self.conv3(x))))  \n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.gap(x)  # Global Average Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten before FC layer\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)  # Final output layer (logits)\n",
    "\n",
    "        x = x.view(-1, self.output_dim, 2)\n",
    "        \n",
    "        return F.softmax(x, dim=-1)  # No sigmoid, use BCEWithLogitsLoss()\n",
    "    \n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=7):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(20, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(21, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 3 * 3, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 3 * 3)\n",
    "        x = self.classifier(x)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "506c1df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, output_dim=19, dropout_prob=0.5):\n",
    "        super(RNNFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout_prob = dropout_prob\n",
    "\n",
    "        # Encoder CNN\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1, 16, kernel_size=3, stride=1, padding=1),  # [batch*seq, 16, 32, 32]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # [batch*seq, 16, 16, 16]\n",
    "            nn.Dropout2d(self.dropout_prob),\n",
    "\n",
    "            nn.ConvTranspose2d(16, 32, kernel_size=3, stride=1, padding=1),  # [batch*seq, 32, 16, 16]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),  # [batch*seq, 32, 8, 8]\n",
    "            nn.Dropout2d(self.dropout_prob),\n",
    "        )\n",
    "\n",
    "        # Flattened CNN feature size: 32 * 8 * 8 = 2048\n",
    "        cnn_output_size = 32 * 8 * 8\n",
    "        self.rnn_hidden_size = 128\n",
    "\n",
    "        # RNN for sequence processing\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=cnn_output_size,\n",
    "            hidden_size=self.rnn_hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout_prob\n",
    "        )\n",
    "\n",
    "        # Dropout after RNN\n",
    "        self.rnn_dropout = nn.Dropout(self.dropout_prob)\n",
    "\n",
    "        # Classification head (multi-label)\n",
    "        self.fc_class = nn.Linear(self.rnn_hidden_size, self.output_dim)\n",
    "\n",
    "#         # Decoder (Deconvolution for reconstruction)\n",
    "#         self.decoder_fc = nn.Linear(self.rnn_hidden_size, cnn_output_size)\n",
    "\n",
    "#         self.deconv = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),  # Upsample to [batch*seq, 16, 16, 16]\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),   # Upsample to [batch*seq, 1, 32, 32]\n",
    "#             nn.Sigmoid()  # Assuming image reconstruction between 0-1\n",
    "#         )\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, h, w = x.size()\n",
    "\n",
    "        # CNN Encoder\n",
    "        x_reshaped = x.view(batch_size * seq_len, 1, h, w)\n",
    "        encoded = self.cnn(x_reshaped)  # [batch*seq, 32, 8, 8]\n",
    "        \n",
    "        # Flatten for RNN\n",
    "        features = encoded.view(batch_size, seq_len, -1)  # [batch, seq, 2048]\n",
    "\n",
    "        # RNN Processing\n",
    "        rnn_out, (h_n, c_n) = self.rnn(features)\n",
    "\n",
    "        # Take the last hidden state\n",
    "        last_hidden = rnn_out[:, -1, :]  # [batch, rnn_hidden_size]\n",
    "        last_hidden_dropout = self.rnn_dropout(last_hidden)\n",
    "\n",
    "        # Classification output (multi-label)\n",
    "        class_output = torch.sigmoid(self.fc_class(last_hidden_dropout))  # [batch, output_dim]\n",
    "\n",
    "#         # Reconstruction output\n",
    "#         decoder_input = self.decoder_fc(last_hidden)  # [batch, 2048]\n",
    "#         decoder_input = decoder_input.view(batch_size, 32, 8, 8)  # Reshape to feature map\n",
    "#         recon_output = self.deconv(decoder_input)  # [batch, 1, 32, 32]\n",
    "\n",
    "        return class_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c3edd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02db2a1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0b7667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c69b98de",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# # my_dataset = get_dataset(blur_file, label_file)\n",
    "\n",
    "# # Define split ratio\n",
    "# train_size = int(0.8 * len(my_dataset))  # 80% training\n",
    "# test_size = len(my_dataset) - train_size  # 20% testing\n",
    "\n",
    "# # Randomly split dataset\n",
    "# train_dataset, test_dataset = random_split(my_dataset, [train_size, test_size])\n",
    "\n",
    "# # Print dataset sizes\n",
    "# print(f\"Total samples: {len(my_dataset)}\")\n",
    "# print(f\"Training samples: {len(train_dataset)}\")\n",
    "# print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c57727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "# # Define DataLoaders for training and testing\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle training data\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)  # No shuffle for testing\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CubeModel(21, 19).to(device)\n",
    "# model = AlexNet(7).to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=0.000001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a8db70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: config loaded from /data/users2/yxiao11/mangoDB/wirehead/examples/satellite/config.yaml\n",
      "Dataset: Data is ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = MongoTupleheadDataset(config_path = \"/data/users2/yxiao11/mangoDB/wirehead/examples/satellite/config.yaml\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511f55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset.__getitem__([0])\n",
    "cube = torch.cat([t.unsqueeze(0) for t in [b[0] for b in batch]], dim=0)\n",
    "labels = torch.cat([t.unsqueeze(0) for t in [b[1] for b in batch]], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a082a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cube[0][0].detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88419d59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d3545e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a379890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output  # For updating plots in Jupyter\n",
    "\n",
    "# Initialize lists for tracking loss\n",
    "my_train_loss = []\n",
    "my_test_loss = []\n",
    "\n",
    "# Create the figure for plotting\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"Iterations\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_title(\"Live Training Loss\")\n",
    "\n",
    "num_samples = dataset.__len__()\n",
    "model.train()\n",
    "my_loss = []\n",
    "iteration = 0\n",
    "\n",
    "num_epoch = int(num_samples/batch_size)\n",
    "while True:  # Modify with a proper stopping condition (e.g., fixed number of iterations)\n",
    "    \n",
    "    running_loss = 0\n",
    "    index = np.random.permutation(np.arange(num_samples))\n",
    "    for i in range(num_epoch):   \n",
    "        \n",
    "        batch = dataset.__getitem__(index[i*batch_size: (i+1)*batch_size])\n",
    "        cube = torch.cat([t.unsqueeze(0) for t in [b[0] for b in batch]], dim=0)\n",
    "        labels = torch.cat([t.unsqueeze(0) for t in [b[1] for b in batch]], dim=0)\n",
    "        cube, labels = cube.to(device, non_blocking=True), labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(cube)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Append the new loss\n",
    "    my_loss.append(running_loss/num_epoch)\n",
    "\n",
    "    # Print loss after each iteration\n",
    "#     print(f\"Iteration {iteration+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # âœ… Update the plot dynamically in Jupyter Notebook\n",
    "    clear_output(wait=True)  # Clear previous plot to update dynamically\n",
    "    ax.clear()  # Clear the axis to redraw\n",
    "    ax.plot(my_loss, label=\"Training Loss\", color=\"blue\")\n",
    "    ax.set_xlabel(\"Iterations\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_title(\"Live Training Loss\")\n",
    "#     ax.legend()\n",
    "    display(fig)  # Re-display updated plot\n",
    "\n",
    "    iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f86457d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66eac923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b2fed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39f340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c26cfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd78270",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaa81ac",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_train_loss = []\n",
    "# my_test_loss = []\n",
    "# num_epochs=300\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training Phase\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "\n",
    "#     for cube, labels in train_loader:\n",
    "#         cube, labels = cube.to(device), labels.to(device)\n",
    "\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(cube)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     avg_train_loss = running_loss / len(train_loader)\n",
    "#     my_train_loss.append(avg_train_loss)\n",
    "    \n",
    "#     # Testing Phase\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "\n",
    "\n",
    "#     with torch.no_grad():  # Disable gradients for validation/testing\n",
    "#         for cube, labels in test_loader:\n",
    "#             cube, labels = cube.to(device), labels.to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(cube)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item()\n",
    "            \n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader)\n",
    "#     my_test_loss.append(avg_test_loss)\n",
    "    \n",
    "#     # Print results for this epoch\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb4942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba9cd6",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_train_loss = []\n",
    "# my_test_loss = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training Phase\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for batch_idx, (cube, labels) in enumerate(train_loader):\n",
    "#         cube, labels = cube.to(device), labels.to(device)\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         outputs = model(cube)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     avg_train_loss = running_loss / len(train_loader)\n",
    "#     my_train_loss.append(avg_train_loss)\n",
    "\n",
    "#     # Testing Phase\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (cube, labels) in enumerate(test_loader):\n",
    "#             cube, labels = cube.to(device), labels.to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(cube)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item()\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader)\n",
    "#     my_test_loss.append(avg_test_loss)\n",
    "\n",
    "\n",
    "\n",
    "#     # Print results for this epoch\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476212a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c512f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc26cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e25986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}_train_loss.npy\",\n",
    "#         np.array(my_train_loss))\n",
    "        \n",
    "# np.save(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}_test_loss.npy\",\n",
    "#         np.array(my_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e443c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4e849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}.pth\")\n",
    "# print(\"Entire model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf284bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to smooth the loss curve using Exponential Moving Average (EMA)\n",
    "def smooth_curve(data, weight=0.9):\n",
    "    smoothed = []\n",
    "    last = data[0]  # Initialize with the first value\n",
    "    for point in data:\n",
    "        last = weight * last + (1 - weight) * point  # EMA formula\n",
    "        smoothed.append(last)\n",
    "    return smoothed\n",
    "\n",
    "# Smooth both training and test loss\n",
    "smooth_train_loss = smooth_curve(my_train_loss, weight=0.9)\n",
    "smooth_test_loss = smooth_curve(my_test_loss, weight=0.9)\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Plot original training loss (faint color)\n",
    "plt.plot(my_train_loss, label=\"Train Loss (Original)\", color=\"royalblue\", alpha=0.3, linewidth=1)\n",
    "\n",
    "# Plot smoothed training loss\n",
    "plt.plot(smooth_train_loss, label=\"Train Loss (Smoothed)\", color=\"royalblue\", linewidth=2)\n",
    "\n",
    "# Plot original test loss (faint color)\n",
    "plt.plot(my_test_loss, label=\"Test Loss (Original)\", color=\"darkorange\", alpha=0.3, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "# Plot smoothed test loss\n",
    "plt.plot(smooth_test_loss, label=\"Test Loss (Smoothed)\", color=\"darkorange\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "# Improve visualization\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.title(\"Training & Validation Loss Curve\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)  # Add grid for better readability\n",
    "\n",
    "plt.savefig(f'/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}.png', \n",
    "            dpi=300, \n",
    "            bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b63831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d996dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data, label = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(\n",
    "#     model.cpu(), sample_data, \"cube_model.onnx\",\n",
    "#     input_names=[\"input\"], output_names=[\"output\"],\n",
    "#     dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "#     opset_version=11\n",
    "# )\n",
    "\n",
    "# print(\"Model saved as cube_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f9754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e401b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a3dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7b419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bfa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716f8aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab0e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type = 'mixed'\n",
    "data_type = 'Pristine'\n",
    "# data_type = 'Irradiated'\n",
    "\n",
    "blur_dir = '/data/users2/yxiao11/model/satellite_project/data/' +data_type + '/blur_cube/'\n",
    "spectral_dir = '/data/users2/yxiao11/model/satellite_project/data/' +data_type + '/spectral_cube/'\n",
    "label_dir = '/data/users2/yxiao11/model/satellite_project/data/' +data_type + '/label/'\n",
    "\n",
    "blur_file = []\n",
    "label_file = []\n",
    "spectral_file = []\n",
    "for i in range(len(os.listdir(blur_dir))):\n",
    "    blur_file.append(blur_dir + f\"{i}.npy\")\n",
    "    label_file.append(label_dir + f\"{i}.npy\")\n",
    "    spectral_file.append(spectral_dir + f\"{i}.npy\")\n",
    "    \n",
    "# Load dataset\n",
    "my_dataset = get_dataset(blur_file, label_file)\n",
    "\n",
    "# Define split ratio\n",
    "train_size = int(0.8 * len(my_dataset))  # 80% training\n",
    "test_size = len(my_dataset) - train_size  # 20% testing\n",
    "\n",
    "# Randomly split dataset\n",
    "train_dataset, test_dataset = random_split(my_dataset, [train_size, test_size])\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total samples: {len(my_dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e96f1952",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2268667/2649455779.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}.pth\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CubeModel(\n",
       "  (conv1): Conv2d(21, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (gap): AdaptiveAvgPool2d(output_size=1)\n",
       "  (fc1): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_type = 'mixed'\n",
    "# data_type = 'Pristine'\n",
    "# data_type = 'Irradiated'\n",
    "\n",
    "model = torch.load(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}.pth\")\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e99dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98df3d5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "num_samples = dataset.__len__()\n",
    "model.train()\n",
    "my_loss = []\n",
    "iteration = 0\n",
    "\n",
    "num_epoch = int(num_samples/batch_size)\n",
    "# while True:  # Modify with a proper stopping condition (e.g., fixed number of iterations)\n",
    "    \n",
    "running_loss = 0\n",
    "index = np.random.permutation(np.arange(num_samples))\n",
    "for i in range(num_epoch):   \n",
    "#     print(i)\n",
    "    batch = dataset.__getitem__(index[i*batch_size: (i+1)*batch_size])\n",
    "    cube = torch.cat([t.unsqueeze(0) for t in [b[0] for b in batch]], dim=0)\n",
    "    labels = torch.cat([t.unsqueeze(0) for t in [b[1] for b in batch]], dim=0)\n",
    "    cube, labels = cube.to(device, non_blocking=True), labels.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0b80bb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b468c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45130888",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset.__getitem__([int(np.random.choice(index))])\n",
    "cube = torch.cat([t.unsqueeze(0) for t in [b[0] for b in batch]], dim=0)\n",
    "labels = torch.cat([t.unsqueeze(0) for t in [b[1] for b in batch]], dim=0)\n",
    "cube, labels = cube.to(device, non_blocking=True), labels.to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c1d4f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]]], device='cuda:0', grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.round(model(cube))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f05ec80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [1., 0.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.],\n",
       "         [0., 1.]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c1344",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5df45fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c224da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee924c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "label_list = []\n",
    "preds_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (cube, labels) in enumerate(test_loader):\n",
    "        cube, labels = cube.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = torch.round(model(cube)).cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label_list.append(labels[i].detach().numpy())\n",
    "            preds_list.append(outputs[i].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74abda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_loss = np.load(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}_train_loss.npy\")\n",
    "        \n",
    "my_test_loss = np.load(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}_test_loss.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to smooth the loss curve using Exponential Moving Average (EMA)\n",
    "def smooth_curve(data, weight=0.9):\n",
    "    smoothed = []\n",
    "    last = data[0]  # Initialize with the first value\n",
    "    for point in data:\n",
    "        last = weight * last + (1 - weight) * point  # EMA formula\n",
    "        smoothed.append(last)\n",
    "    return smoothed\n",
    "\n",
    "# Smooth both training and test loss\n",
    "smooth_train_loss = smooth_curve(my_train_loss, weight=0.9)\n",
    "smooth_test_loss = smooth_curve(my_test_loss, weight=0.9)\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot original training loss (faint color)\n",
    "plt.plot(my_train_loss, label=\"Train Loss (Original)\", color=\"royalblue\", alpha=0.3, linewidth=1)\n",
    "\n",
    "# Plot smoothed training loss\n",
    "plt.plot(smooth_train_loss, label=\"Train Loss (Smoothed)\", color=\"royalblue\", linewidth=2)\n",
    "\n",
    "# Plot original test loss (faint color)\n",
    "plt.plot(my_test_loss, label=\"Test Loss (Original)\", color=\"darkorange\", alpha=0.3, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "# Plot smoothed test loss\n",
    "plt.plot(smooth_test_loss, label=\"Test Loss (Smoothed)\", color=\"darkorange\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "# Improve visualization\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.title(f\"{data_type}\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)  # Add grid for better readability\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "part_list = ['antenna', 'antenna', 'antenna',\n",
    "            'body top', 'body top', 'body top',\n",
    "             'body bottom', 'body bottom', 'body bottom',\n",
    "             'lateral surface', 'lateral surface','lateral surface',\n",
    "             'lateral surface','lateral surface','lateral surface',\n",
    "             'Connectors', 'Panel', 'Panel', 'Panel'\n",
    "            ]\n",
    "\n",
    "# Stack predictions and true labels\n",
    "y_true = np.vstack(label_list)  # True labels (multi-label)\n",
    "y_pred = np.vstack(preds_list)  # Predicted labels\n",
    "\n",
    "# Compute multi-label confusion matrices (one per class)\n",
    "conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ðŸ“Œ **1ï¸âƒ£ Better Confusion Matrix Layout (Grid instead of Single Row)**\n",
    "num_classes = len(conf_matrices)  # Should be 19\n",
    "cols = 5  # Set number of columns for grid\n",
    "rows = (num_classes // cols) + (num_classes % cols > 0)  # Auto adjust rows\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))  # Dynamic grid size\n",
    "axes = axes.flatten()  # Flatten grid for easy indexing\n",
    "\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i], cbar=False)\n",
    "    axes[i].set_title(f\"Material {i+1}\\n{part_list[i]}\", fontsize=15)\n",
    "    axes[i].set_xlabel(\"Predicted\", fontsize=10)\n",
    "    axes[i].set_ylabel(\"Actual\", fontsize=10)\n",
    "    axes[i].tick_params(axis='both', labelsize=6)\n",
    "\n",
    "# Hide unused subplots (in case 19 is not a perfect grid)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy_per_class = np.mean(y_true == y_pred, axis=0)  # Per-class accuracy\n",
    "hamming = hamming_loss(y_true, y_pred)  # Hamming loss\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)  # Per-class metrics\n",
    "macro_f1 = np.mean(f1)  # Macro F1-score\n",
    "micro_f1 = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")[2]  # Micro F1-score\n",
    "jaccard = jaccard_score(y_true, y_pred, average=\"samples\")  # Jaccard similarity\n",
    "\n",
    "# Display Metrics\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro F1-Score: {micro_f1:.4f}\")\n",
    "print(f\"Jaccard Similarity Score: {jaccard:.4f}\")\n",
    "\n",
    "# Plot accuracy, precision, recall, and F1-score per class\n",
    "metrics = {\"Accuracy\": accuracy_per_class, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for metric, values in metrics.items():\n",
    "    ax.plot(range(1, len(values)+1), values, marker=\"o\", label=metric)\n",
    "\n",
    "ax.set_xticks(np.arange(19)+1)\n",
    "ax.set_xlabel(\"Material Class\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Performance Metrics per Class\",fontsize=16, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef4d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179f0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c952ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_recall_fscore_support, hamming_loss, jaccard_score\n",
    "\n",
    "# Define material categories\n",
    "part_list = [\n",
    "    'Antenna', 'Antenna', 'Antenna',\n",
    "    'Body Top', 'Body Top', 'Body Top',\n",
    "    'Body Bottom', 'Body Bottom', 'Body Bottom',\n",
    "    'Lateral Surface', 'Lateral Surface', 'Lateral Surface',\n",
    "    'Lateral Surface', 'Lateral Surface', 'Lateral Surface',\n",
    "    'Connectors', 'Panel', 'Panel', 'Panel'\n",
    "]\n",
    "\n",
    "# Stack predictions and true labels\n",
    "y_true = np.vstack(label_list)  # True labels (multi-label)\n",
    "y_pred = np.vstack(preds_list)  # Predicted labels\n",
    "\n",
    "# Compute multi-label confusion matrices (one per class)\n",
    "conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# ðŸ“Œ **1ï¸âƒ£ Better Confusion Matrix Layout (Grid View)**\n",
    "num_classes = len(conf_matrices)  \n",
    "cols = 5  \n",
    "rows = (num_classes // cols) + (num_classes % cols > 0)  \n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))\n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i], cbar=False)\n",
    "    axes[i].set_title(f\"Material {i+1}\\n{part_list[i]}\", fontsize=15)\n",
    "    axes[i].set_xlabel(\"Predicted\", fontsize=10)\n",
    "    axes[i].set_ylabel(\"Actual\", fontsize=10)\n",
    "    axes[i].tick_params(axis='both', labelsize=6)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ðŸ“Œ **2ï¸âƒ£ Compute Classification Metrics with Clearer Names**\n",
    "accuracy_per_class = np.mean(y_true == y_pred, axis=0)  # Accuracy per class\n",
    "hamming = hamming_loss(y_true, y_pred)  # Average label error rate\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)  # Per-class scores\n",
    "macro_f1 = np.mean(f1)  # Overall Class Balance Score\n",
    "micro_f1 = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")[2]  # Weighted Class Balance Score\n",
    "jaccard = jaccard_score(y_true, y_pred, average=\"samples\")  # Label Overlap Score\n",
    "\n",
    "# ðŸ“Œ **3ï¸âƒ£ Print More Understandable Metrics**\n",
    "print(f\"Average Label Error Rate (Hamming Loss): {hamming:.4f}\")\n",
    "print(f\"Overall Class Balance Score (Macro F1-Score): {macro_f1:.4f}\")\n",
    "print(f\"Weighted Class Balance Score (Micro F1-Score): {micro_f1:.4f}\")\n",
    "print(f\"Label Overlap Score (Jaccard Similarity Score): {jaccard:.4f}\")\n",
    "\n",
    "# ðŸ“Œ **4ï¸âƒ£ Plot Metrics for Each Class**\n",
    "metrics = {\n",
    "    \"Accuracy per Material\": accuracy_per_class,\n",
    "    \"Label Correctness Rate (Precision)\": precision,\n",
    "    \"Label Coverage Rate (Recall)\": recall,\n",
    "    \"Prediction Quality Score (F1-Score)\": f1\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for metric, values in metrics.items():\n",
    "    ax.plot(range(1, len(values)+1), values, marker=\"o\", label=metric)\n",
    "\n",
    "ax.set_xticks(np.arange(len(part_list))+1)\n",
    "ax.set_xticklabels(part_list, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_xlabel(\"Material Type\", fontsize=12)\n",
    "ax.set_ylabel(\"Score\", fontsize=12)\n",
    "ax.set_title(\"ðŸ“Š Performance Metrics per Material\", fontsize=16, fontweight=\"bold\")\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80099052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cae3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d85654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829ae1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dae8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807613a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa95e78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc7917",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for cube, labels in train_loader:\n",
    "#         cube, labels = cube.to(device), labels.to(device)\n",
    "\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "        \n",
    "#         # Forward pass\n",
    "#         outputs = model(cube)\n",
    "#         loss = criterion(outputs, labels)\n",
    "        \n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         scheduler.step()\n",
    "        \n",
    "#         running_loss += loss.item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713cb6ac",
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# model.eval()  # Set model to evaluation mode\n",
    "# total_test_loss = 0\n",
    "\n",
    "# with torch.no_grad():  # No gradient calculation during testing\n",
    "#     for inputs, labels in test_loader:\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "#         outputs = model(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         total_test_loss += loss.item()\n",
    "\n",
    "# print(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ac40da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0b03b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ddf50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0b88f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data, label = next(iter(test_loader))\n",
    "sample_data = sample_data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc5623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.round(model(sample_data)))\n",
    "\n",
    "print(torch.round(model(sample_data)) - label.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ab766",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = (torch.round(model(sample_data)) - label.to(device)).cpu().detach().numpy()\n",
    "plt.imshow(to_show)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d782c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07a618e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2963ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "# m = np.load('/Users/yaorongxiao/Desktop/satellite/spectral_cube.npy')\n",
    "m = np.load(f'./data/sim_data/spectral_cube/{i}.npy')\n",
    "\n",
    "print(np.load(f'./data/sim_data/label/{i}.npy'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22d8226",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(21,1, figsize=(10,100))\n",
    "\n",
    "for i in range(21):\n",
    "    ax[i].imshow(m[:,:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f2f3f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
