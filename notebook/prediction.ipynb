{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc2c4515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from itertools import permutations \n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_recall_fscore_support, hamming_loss, jaccard_score\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import trimesh\n",
    "\n",
    "from moduler import *\n",
    "\n",
    "import sys\n",
    "sys.path.append('/data/users2/yxiao11/mangoDB/wirehead')\n",
    "from wirehead import WireheadGenerator\n",
    "from wirehead import MongoTupleheadDataset, MongoheadDataset\n",
    "\n",
    "torch.set_printoptions(sci_mode=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ff05c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38483ab0",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data_type = 'mixed'\n",
    "# data_type = 'Pristine'\n",
    "# data_type = 'Irradiated'\n",
    "\n",
    "blur_dir = '/data/users2/yxiao11/model/satellite_project/database/' +data_type + '/blur_cube/'\n",
    "# spectral_dir = '/data/users2/yxiao11/model/satellite_project/database/' +data_type + '/spectral_cube/'\n",
    "label_dir = '/data/users2/yxiao11/model/satellite_project/database/' +data_type + '/label/'\n",
    "\n",
    "blur_file = []\n",
    "label_file = []\n",
    "spectral_file = []\n",
    "for i in range(len(os.listdir(blur_dir))):\n",
    "    blur_file.append(blur_dir + f\"{i}.npy\")\n",
    "    label_file.append(label_dir + f\"{i}.npy\")\n",
    "#     spectral_file.append(spectral_dir + f\"{i}.npy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accda149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edf49e8a",
   "metadata": {
    "code_folding": [
     78,
     138,
     174,
     195
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "class get_dataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_dir = labels_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "#         print(idx)\n",
    "        ipt = torch.from_numpy(np.load(self.data_dir[idx]))\n",
    "      \n",
    "#         ipt = ipt.permute(2, 0, 1)\n",
    "        \n",
    "        label_index = torch.from_numpy(np.load(self.labels_dir[idx]))-1\n",
    "        \n",
    "        label = torch.tensor([0, 1]).repeat(19, 1)\n",
    "        label[label_index] = torch.tensor([1, 0])\n",
    "        \n",
    "#         label = torch.zeros(19)\n",
    "#         label[label_index] = 1\n",
    "        \n",
    "        return ipt.float(), label.float()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85892d90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c69b98de",
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 1000\n",
      "Training samples: 800\n",
      "Testing samples: 200\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "my_dataset = get_dataset(blur_file, label_file)\n",
    "\n",
    "# Define split ratio\n",
    "train_size = int(0.8 * len(my_dataset))  # 80% training\n",
    "test_size = len(my_dataset) - train_size  # 20% testing\n",
    "\n",
    "# Randomly split dataset\n",
    "train_dataset, test_dataset = random_split(my_dataset, [train_size, test_size])\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Total samples: {len(my_dataset)}\")\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1c57727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epochs = 100\n",
    "batch_size = 100\n",
    "\n",
    "# # Define DataLoaders for training and testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # Shuffle training data\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)  # No shuffle for testing\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = CubeModel(52, 19).to(device)\n",
    "# model = AlexNet(7).to(device)\n",
    "# model = unet().to(device)\n",
    "model = RNNFeatureExtractor().to(device)\n",
    "\n",
    "# Define Loss Function and Optimizer\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion = torch.nn.MSELoss()\n",
    "# criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=30, eta_min=0.000001)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fb10a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipt, y = next(iter(train_loader))\n",
    "ipt = ipt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af1c43a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 52, 32, 32])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ipt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b2c42f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 19, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39f340",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfaa81ac",
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 0.0504, Test Loss: 0.0000\n",
      "Epoch 2/1000, Train Loss: 0.0000, Test Loss: 0.0000\n",
      "Epoch 3/1000, Train Loss: 0.0000, Test Loss: 0.0000\n",
      "Epoch 4/1000, Train Loss: 0.0000, Test Loss: 0.0000\n",
      "Epoch 5/1000, Train Loss: 0.0000, Test Loss: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(cube)\n\u001b[1;32m     40\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 41\u001b[0m         test_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m avg_test_loss \u001b[38;5;241m=\u001b[39m test_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(test_loader)\n\u001b[1;32m     45\u001b[0m my_test_loss\u001b[38;5;241m.\u001b[39mappend(avg_test_loss)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "my_train_loss = []\n",
    "my_test_loss = []\n",
    "num_epochs=1000\n",
    "for epoch in range(num_epochs):\n",
    "    # Training Phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for cube, labels in train_loader:\n",
    "        cube, labels = cube.to(device), labels.to(device)\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(cube)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    my_train_loss.append(avg_train_loss)\n",
    "    \n",
    "    # Testing Phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "\n",
    "\n",
    "    with torch.no_grad():  # Disable gradients for validation/testing\n",
    "        for cube, labels in test_loader:\n",
    "            cube, labels = cube.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(cube)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    my_test_loss.append(avg_test_loss)\n",
    "    \n",
    "    # Print results for this epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906f7b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51b694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4e995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345a700",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ba9cd6",
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# my_train_loss = []\n",
    "# my_test_loss = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Training Phase\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for batch_idx, (cube, labels) in enumerate(train_loader):\n",
    "#         cube, labels = cube.to(device), labels.to(device)\n",
    "#         # Zero gradients\n",
    "#         optimizer.zero_grad()\n",
    "#         # Forward pass\n",
    "#         outputs = model(cube)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         # Backward pass and optimization\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "        \n",
    "#     avg_train_loss = running_loss / len(train_loader)\n",
    "#     my_train_loss.append(avg_train_loss)\n",
    "\n",
    "#     # Testing Phase\n",
    "#     model.eval()\n",
    "#     test_loss = 0.0\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for batch_idx, (cube, labels) in enumerate(test_loader):\n",
    "#             cube, labels = cube.to(device), labels.to(device)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             outputs = model(cube)\n",
    "#             loss = criterion(outputs, labels)\n",
    "#             test_loss += loss.item()\n",
    "\n",
    "#     avg_test_loss = test_loss / len(test_loader)\n",
    "#     my_test_loss.append(avg_test_loss)\n",
    "\n",
    "\n",
    "\n",
    "#     # Print results for this epoch\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476212a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751c512f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc26cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e25986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}_train_loss.npy\",\n",
    "#         np.array(my_train_loss))\n",
    "        \n",
    "# np.save(f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}_test_loss.npy\",\n",
    "#         np.array(my_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e443c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f4e849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fc10d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, f\"/data/users2/yxiao11/model/satellite_project/resluts_n_model/{data_type}.pth\")\n",
    "# print(\"Entire model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b63831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d996dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data, label = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4d8ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.onnx.export(\n",
    "#     model.cpu(), sample_data, \"cube_model.onnx\",\n",
    "#     input_names=[\"input\"], output_names=[\"output\"],\n",
    "#     dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}},\n",
    "#     opset_version=11\n",
    "# )\n",
    "\n",
    "# print(\"Model saved as cube_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50f9754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e401b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234a3dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f7b419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15bfa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3e086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcefac7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee924c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b45ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "label_list = []\n",
    "preds_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (cube, labels) in enumerate(test_loader):\n",
    "        cube, labels = cube.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = torch.round(model(cube)).cpu()\n",
    "        labels = labels.cpu()\n",
    "        \n",
    "#         for i in range(batch_size):\n",
    "#             label_list.append(torch.argmin(labels[0], dim=1).detach().numpy())\n",
    "#             preds_list.append(torch.argmin(outputs[0], dim=1).detach().numpy())\n",
    "            \n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label_list.append(labels[i].detach().numpy())\n",
    "            preds_list.append(outputs[i].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e315729",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd8f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to smooth the loss curve using Exponential Moving Average (EMA)\n",
    "def smooth_curve(data, weight=0.9):\n",
    "    smoothed = []\n",
    "    last = data[0]  # Initialize with the first value\n",
    "    for point in data:\n",
    "        last = weight * last + (1 - weight) * point  # EMA formula\n",
    "        smoothed.append(last)\n",
    "    return smoothed\n",
    "\n",
    "# Smooth both training and test loss\n",
    "smooth_train_loss = smooth_curve(my_train_loss, weight=0.9)\n",
    "smooth_test_loss = smooth_curve(my_test_loss, weight=0.9)\n",
    "\n",
    "# Create the figure\n",
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "# Plot original training loss (faint color)\n",
    "plt.plot(my_train_loss, label=\"Train Loss (Original)\", color=\"royalblue\", alpha=0.3, linewidth=1)\n",
    "\n",
    "# Plot smoothed training loss\n",
    "plt.plot(smooth_train_loss, label=\"Train Loss (Smoothed)\", color=\"royalblue\", linewidth=2)\n",
    "\n",
    "# Plot original test loss (faint color)\n",
    "plt.plot(my_test_loss, label=\"Test Loss (Original)\", color=\"darkorange\", alpha=0.3, linewidth=1, linestyle=\"--\")\n",
    "\n",
    "# Plot smoothed test loss\n",
    "plt.plot(smooth_test_loss, label=\"Test Loss (Smoothed)\", color=\"darkorange\", linewidth=2, linestyle=\"--\")\n",
    "\n",
    "# Improve visualization\n",
    "plt.xlabel(\"Epochs\", fontsize=14)\n",
    "plt.ylabel(\"Loss\", fontsize=14)\n",
    "plt.title(f\"{data_type}\", fontsize=16, fontweight=\"bold\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.6)  # Add grid for better readability\n",
    "\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb1ed58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27ad910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a28593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4ab95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a5e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "part_list = ['antenna', 'antenna', 'antenna',\n",
    "            'body top', 'body top', 'body top',\n",
    "             'body bottom', 'body bottom', 'body bottom',\n",
    "             'lateral surface', 'lateral surface','lateral surface',\n",
    "             'lateral surface','lateral surface','lateral surface',\n",
    "             'Connectors', 'Panel', 'Panel', 'Panel'\n",
    "            ]\n",
    "\n",
    "# Stack predictions and true labels\n",
    "y_true = np.vstack(label_list)  # True labels (multi-label)\n",
    "y_pred = np.vstack(preds_list)  # Predicted labels\n",
    "\n",
    "# Compute multi-label confusion matrices (one per class)\n",
    "conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 📌 **1️⃣ Better Confusion Matrix Layout (Grid instead of Single Row)**\n",
    "num_classes = len(conf_matrices)  # Should be 19\n",
    "cols = 5  # Set number of columns for grid\n",
    "rows = (num_classes // cols) + (num_classes % cols > 0)  # Auto adjust rows\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))  # Dynamic grid size\n",
    "axes = axes.flatten()  # Flatten grid for easy indexing\n",
    "\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i], cbar=False)\n",
    "    axes[i].set_title(f\"Material {i+1}\\n{part_list[i]}\", fontsize=15)\n",
    "    axes[i].set_xlabel(\"Predicted\", fontsize=10)\n",
    "    axes[i].set_ylabel(\"Actual\", fontsize=10)\n",
    "    axes[i].tick_params(axis='both', labelsize=6)\n",
    "\n",
    "# Hide unused subplots (in case 19 is not a perfect grid)\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compute classification metrics\n",
    "accuracy_per_class = np.mean(y_true == y_pred, axis=0)  # Per-class accuracy\n",
    "hamming = hamming_loss(y_true, y_pred)  # Hamming loss\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)  # Per-class metrics\n",
    "macro_f1 = np.mean(f1)  # Macro F1-score\n",
    "micro_f1 = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")[2]  # Micro F1-score\n",
    "jaccard = jaccard_score(y_true, y_pred, average=\"samples\")  # Jaccard similarity\n",
    "\n",
    "# Display Metrics\n",
    "print(f\"Hamming Loss: {hamming:.4f}\")\n",
    "print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "print(f\"Micro F1-Score: {micro_f1:.4f}\")\n",
    "print(f\"Jaccard Similarity Score: {jaccard:.4f}\")\n",
    "\n",
    "# Plot accuracy, precision, recall, and F1-score per class\n",
    "metrics = {\"Accuracy\": accuracy_per_class, \"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for metric, values in metrics.items():\n",
    "    ax.plot(range(1, len(values)+1), values, marker=\"o\", label=metric)\n",
    "\n",
    "ax.set_xticks(np.arange(19)+1)\n",
    "ax.set_xlabel(\"Material Class\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_title(\"Performance Metrics per Class\",fontsize=16, fontweight=\"bold\")\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ef4d77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9179f0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c952ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score, precision_recall_fscore_support, hamming_loss, jaccard_score\n",
    "\n",
    "# Define material categories\n",
    "part_list = [\n",
    "    'Antenna', 'Antenna', 'Antenna',\n",
    "    'Body Top', 'Body Top', 'Body Top',\n",
    "    'Body Bottom', 'Body Bottom', 'Body Bottom',\n",
    "    'Lateral Surface', 'Lateral Surface', 'Lateral Surface',\n",
    "    'Lateral Surface', 'Lateral Surface', 'Lateral Surface',\n",
    "    'Connectors', 'Panel', 'Panel', 'Panel'\n",
    "]\n",
    "\n",
    "# Stack predictions and true labels\n",
    "y_true = np.vstack(label_list)  # True labels (multi-label)\n",
    "y_pred = np.vstack(preds_list)  # Predicted labels\n",
    "\n",
    "# Compute multi-label confusion matrices (one per class)\n",
    "conf_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# 📌 **1️⃣ Better Confusion Matrix Layout (Grid View)**\n",
    "num_classes = len(conf_matrices)  \n",
    "cols = 5  \n",
    "rows = (num_classes // cols) + (num_classes % cols > 0)  \n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(15, rows * 3))\n",
    "axes = axes.flatten()  \n",
    "\n",
    "for i, cm in enumerate(conf_matrices):\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", ax=axes[i], cbar=False)\n",
    "    axes[i].set_title(f\"Material {i+1}\\n{part_list[i]}\", fontsize=15)\n",
    "    axes[i].set_xlabel(\"Predicted\", fontsize=10)\n",
    "    axes[i].set_ylabel(\"Actual\", fontsize=10)\n",
    "    axes[i].tick_params(axis='both', labelsize=6)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 📌 **2️⃣ Compute Classification Metrics with Clearer Names**\n",
    "accuracy_per_class = np.mean(y_true == y_pred, axis=0)  # Accuracy per class\n",
    "hamming = hamming_loss(y_true, y_pred)  # Average label error rate\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=None)  # Per-class scores\n",
    "macro_f1 = np.mean(f1)  # Overall Class Balance Score\n",
    "micro_f1 = precision_recall_fscore_support(y_true, y_pred, average=\"micro\")[2]  # Weighted Class Balance Score\n",
    "jaccard = jaccard_score(y_true, y_pred, average=\"samples\")  # Label Overlap Score\n",
    "\n",
    "# 📌 **3️⃣ Print More Understandable Metrics**\n",
    "# print(f\"Average Label Error Rate (Hamming Loss): {hamming:.4f}\")\n",
    "# print(f\"Overall Class Balance Score (Macro F1-Score): {macro_f1:.4f}\")\n",
    "# print(f\"Weighted Class Balance Score (Micro F1-Score): {micro_f1:.4f}\")\n",
    "print(f\"Label Overlap Score (Jaccard Similarity Score): {jaccard:.4f}\")\n",
    "\n",
    "# 📌 **4️⃣ Plot Metrics for Each Class**\n",
    "metrics = {\n",
    "#     \"Accuracy per Material\": accuracy_per_class,\n",
    "#     \"Label Correctness Rate (Precision)\": precision,\n",
    "#     \"Label Coverage Rate (Recall)\": recall,\n",
    "    \"Prediction Quality Score (F1-Score)\": f1\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "for metric, values in metrics.items():\n",
    "    ax.plot(range(1, len(values)+1), values, marker=\"o\", label=metric)\n",
    "\n",
    "ax.set_xticks(np.arange(len(part_list))+1)\n",
    "ax.set_xticklabels(part_list, rotation=45, ha=\"right\", fontsize=9)\n",
    "ax.set_xlabel(\"Material Type\", fontsize=12)\n",
    "ax.set_ylabel(\"Score\", fontsize=12)\n",
    "ax.set_title(\"f1 score per Material\", fontsize=16, fontweight=\"bold\")\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80099052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cae3b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d85654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f829ae1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dae8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2807613a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa95e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
